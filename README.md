# ğŸ“š Retrieval-Augmented Q&A Assistant â€“ Module 1 Project (AAIDC)

This project implements a clean, production-ready Retrieval-Augmented Generation (RAG) system that loads domain documents, chunks them with overlap, embeds them using Google Generative AI, stores vectors in ChromaDB, and generates grounded answers strictly from retrieved context â€” minimizing hallucinations.

Built as part of the Ready Tensor â€“ Agentic AI Developer Certification (Module 1).

---

# ğŸ¯ What Youâ€™ll Build

A fully functional RAG pipeline that can:
Load your own .txt documents
Chunk text using smart sentence-aware logic + overlap
Create embeddings using Google text-embedding-004
Store vectors persistently in ChromaDB
Retrieve the most relevant document chunks for any question
Generate accurate answers using Gemini 2.5 Flash
Avoid hallucinations using strict context-only responses
Handle failures using retry and deduplication mechanisms

## âœ… Project Features
âœ” Loads and preprocesses `.txt` documents  
âœ” Sentence-aware chunking with overlap (prevents context loss)  
âœ” Query preprocessing (lowercasing, trimming, punctuation cleaning)  
âœ” Embedding via Google Generative AI (text-embedding-004)  
âœ” Vector storage & similarity search using ChromaDB  
âœ” Retry logic to handle embedding API timeouts  
âœ” RAG generation using Gemini 2.5 Flash  
âœ” Duplicated chunks removed during context assembly  
âœ” Safe fallback response  

"I don't have enough information from the documents."

---

# ğŸ“ Implementation Steps (Complete Guide)

These steps match your actual functions and file structure.

Step 1 â€” Prepare Your Documents
ğŸ“ Folder: data/
Replace sample documents with your own text files:

data/
â”œâ”€â”€ topic1.txt
â”œâ”€â”€ topic2.txt
â””â”€â”€ topic3.txt

Each file should contain plain text content.

Step 2 â€” Document Loading

ğŸ“„ File: src/app.py
ğŸ”§ Function: load_documents()

What it does:
Reads every .txt file in /data
Strips whitespace + attaches metadata
Returns a clean list of documents for vectorization

Step 3 â€” Text Chunking With Overlap

ğŸ“„ File: src/vectordb.py
ğŸ”§ Function: chunk_text()

Your system uses:
Sentence-aware splitting
~500 character target size
40-character overlap between chunks
Natural punctuation-based boundaries

This significantly improves retrieval quality and context continuity.

Step 4 â€” Document Ingestion

ğŸ“„ File: src/vectordb.py
ğŸ”§ Function: add_documents()

What happens:
Text is chunked
Embeddings created using text-embedding-004
Chunks stored in ChromaDB with metadata:
source filename
chunk index
length
Retry logic handles embedding 504/timeouts reliably.

Step 5 â€” Similarity Search

ğŸ“„ File: src/vectordb.py
ğŸ”§ Function: search()

The function:
Embeds user query
Performs vector search on stored embeddings
Returns top-k most relevant chunks
Outputs clean structured results for the pipeline

Step 6 â€” RAG Prompt Template

ğŸ“„ File: src/app.py

Your prompt enforces:
Answers only from retrieved context
2â€“4 sentence focused responses
No hallucination
Optional source citation
This keeps responses safe and grounded.

Step 7 â€” The RAG Query Pipeline

ğŸ“„ File: src/app.py
ğŸ”§ Function: query()

Pipeline steps:
Embed user question
Retrieve chunks
Deduplicate by (source, chunk_index)
Build final context
Pass prompt to Gemini

Return:
grounded answer
context chunks
metadata
unique source list
Debug view available using:
--dump-context


## ğŸ“ Project Structure

![alt text](<Project Structure-1.png>)


### âœ… âš™ï¸ Setup & Installation

## 1ï¸âƒ£ Create & activate virtual environment
```powershell
python -m venv .venv
.\.venv\Scripts\Activate.ps1

2ï¸âƒ£ Install dependencies
python -m pip install -r requirements.txt

3ï¸âƒ£ Configure Environment Variables
Create a .env file in the project root (same folder as src/ and requirements.txt):
GOOGLE_API_KEY=your_google_api_key_here
âœ… .env is ignored by Git (protected)
âœ… .env.example is provided for reference


âœ… Running the Application
python .\src\app.py --q "What are VAEs used for?"
python .\src\app.py --q "What is the difference between VAEs and autoencoders?"
python .\src\app.py --q "How do transformers model long-range dependencies?"

To view retrieved chunks (debug mode):
python .\src\app.py --q "What are VAEs used for?" --dump-context

# ğŸ§ª Experiments & Evaluation (Summary)

ğŸ”¬ 14.1 Experimental Setup

CPU: Intel Core i3
RAM: 8 GB
OS: Windows
Embedding Model: text-embedding-004
Vector DB: ChromaDB
LLM: Gemini 2.5 Flash
Chunk size: 500 chars + 40 overlap
Retrieval: top-k = 3

ğŸ§­ 14.2 Evaluation Methodology

Evaluated based on:
Retrieval relevance
Grounding accuracy
Overlap continuity impact
Robustness to malformed queries
Behavior under failure modes
Deduplication correctness

ğŸ“Š 14.3 Metrics Used

Manual relevance scoring
Grounding accuracy (Yes/No)
Overlap continuity score
Robustness & retry success
Zero hallucination validation

# ğŸ§° Tech Stack
Python 3.9+
LangChain Core
LangChain Google GenAI
Google Embedding Model: text-embedding-004
Gemini 2.5 Flash (LLM)
ChromaDB (vector storage)
dotenv for secure environment configuration

# ğŸ“„ License
This project is licensed under the MIT License.
See the [LICENSE](https://github.com/sbm-11-SFDC/rt-aaidc-project1-template/blob/main/LICENSE) file for details

ğŸ‘¤ Author
Suraj Mahale
AI & Salesforce Developer
GitHub:https://github.com/sbm-11-SFDC
