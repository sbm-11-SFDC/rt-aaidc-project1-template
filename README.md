# ğŸ“š Retrieval-Augmented Q&A Assistant â€“ Module 1 Project (AAIDC)

This project implements a clean, production-ready Retrieval-Augmented Generation (RAG) system that loads domain documents, chunks them with overlap, embeds them using Google Generative AI, stores vectors in ChromaDB, and generates grounded answers strictly from retrieved context â€” minimizing hallucinations.

Built as part of the Ready Tensor â€“ Agentic AI Developer Certification (Module 1).

---

# ğŸ¯ What Youâ€™ll Build

A fully functional RAG pipeline that can:
Load your own .txt documents
Chunk text using smart sentence-aware logic + overlap
Create embeddings using Google text-embedding-004
Store vectors persistently in ChromaDB
Retrieve the most relevant document chunks for any question
Generate accurate answers using Gemini 2.5 Flash
Avoid hallucinations using strict context-only responses
Handle failures using retry and deduplication mechanisms

## âœ… Project Features
âœ” Loads and preprocesses `.txt` documents  
âœ” Sentence-aware chunking with overlap (prevents context loss)  
âœ” Query preprocessing (lowercasing, trimming, punctuation cleaning)  
âœ” Embedding via Google Generative AI (text-embedding-004)  
âœ” Vector storage & similarity search using ChromaDB  
âœ” Retry logic to handle embedding API timeouts  
âœ” RAG generation using Gemini 2.5 Flash  
âœ” Duplicated chunks removed during context assembly  
âœ” Safe fallback response  

"I don't have enough information from the documents."

---

# ğŸ“ Implementation Steps (Complete Guide)

These steps match your actual functions and file structure.

Step 1 â€” Prepare Your Documents

ğŸ“ Folder: data/
Replace sample documents with your own text files:

data/
â”œâ”€â”€ topic1.txt
â”œâ”€â”€ topic2.txt
â””â”€â”€ topic3.txt


Each file should contain plain text you want your RAG system to search.

Step 2 â€” Document Loading

ğŸ“„ File: src/app.py
ğŸ”§ Function: load_documents()

What it does:
Reads every .txt file inside /data
Strips whitespace
Attaches metadata (source: filename)
Returns a structured list for ingestion

Step 3 â€” Text Chunking With Overlap

ğŸ“„ File: src/vectordb.py
ğŸ”§ Function: chunk_text()

Your implementation includes:
Sentence-aware splitting
Approx. 500-character chunk size
40-character overlap to preserve continuity
Natural punctuation-based segmentation
This greatly improves retrieval quality.

Step 4 â€” Document Ingestion

ğŸ“„ File: src/vectordb.py
ğŸ”§ Function: add_documents()

What happens internally:
Documents are chunked
Embeddings created using text-embedding-004
Stored in ChromaDB with metadata:
source
chunk_index
length
Retry logic handles API timeouts (504 errors)

Step 5 â€” Similarity Search

ğŸ“„ File: src/vectordb.py
ğŸ”§ Function: search()

Responsibilities:
Embed the user query
Perform vector similarity search
Retrieve top-k relevant chunks
Return structured results (docs, metadatas, distances)

Step 6 â€” RAG Prompt Template

ğŸ“„ File: src/app.py

Your prompt enforces:
Use only retrieved context
2â€“4 sentence focused answers
No hallucinations
Optional single source citation
This ensures grounded, consistent responses.

Step 7 â€” RAG Query Pipeline

ğŸ“„ File: src/app.py
ğŸ”§ Function: query()

Pipeline steps:
Embed user question
Retrieve relevant chunks
Deduplicate by (source, chunk_index)
Assemble context
Pass prompt to Gemini

Return:
final answer
retrieved chunks
metadata
source list

ğŸ›  Debug mode:

python src/app.py --q "Your Question" --dump-context


## ğŸ“ Project Structure

![alt text](<Project Structure-1.png>)


### âœ… âš™ï¸ Setup & Installation

## 1ï¸âƒ£ Create & activate virtual environment
```powershell
python -m venv .venv
.\.venv\Scripts\Activate.ps1

2ï¸âƒ£ Install dependencies
python -m pip install -r requirements.txt

3ï¸âƒ£ Configure Environment Variables
Create a .env file in the project root (same folder as src/ and requirements.txt):
GOOGLE_API_KEY=your_google_api_key_here
âœ… .env is ignored by Git (protected)
âœ… .env.example is provided for reference


âœ… Running the Application
python .\src\app.py --q "What are VAEs used for?"
python .\src\app.py --q "What is the difference between VAEs and autoencoders?"
python .\src\app.py --q "How do transformers model long-range dependencies?"

To view retrieved chunks (debug mode):
python .\src\app.py --q "What are VAEs used for?" --dump-context

# ğŸ§ª Experiments & Evaluation (Summary)

ğŸ”¬ 14.1 Experimental Setup

CPU: Intel Core i3
RAM: 8 GB
OS: Windows
Embedding Model: text-embedding-004
Vector DB: ChromaDB
LLM: Gemini 2.5 Flash
Chunk size: 500 chars + 40 overlap
Retrieval: top-k = 3

ğŸ§­ 14.2 Evaluation Methodology

Evaluated based on:
Retrieval relevance
Grounding accuracy
Overlap continuity impact
Robustness to malformed queries
Behavior under failure modes
Deduplication correctness

ğŸ“Š 14.3 Metrics Used

Manual relevance scoring
Grounding accuracy (Yes/No)
Overlap continuity score
Robustness & retry success
Zero hallucination validation

# ğŸ§° Tech Stack
Python 3.9+
LangChain Core
LangChain Google GenAI
Google Embedding Model: text-embedding-004
Gemini 2.5 Flash (LLM)
ChromaDB (vector storage)
dotenv for secure environment configuration

# ğŸ“„ License
This project is licensed under the MIT License.
See the [LICENSE] file for details

ğŸ‘¤ Author
Suraj Mahale
AI & Salesforce Developer
GitHub:https://github.com/sbm-11-SFDC
